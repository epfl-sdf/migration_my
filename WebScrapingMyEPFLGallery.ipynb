{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = 'urls_myEpflGallery.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_urls = pd.read_csv(input_)\n",
    "df_urls = df_urls.rename(index=str, columns={'Sites': 'URL'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://innoseed-enac.epfl.ch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://acm.epfl.ch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://adec.epfl.ch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://aesv.epfl.ch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://aphys.epfl.ch</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             URL\n",
       "0  https://innoseed-enac.epfl.ch\n",
       "1             http://acm.epfl.ch\n",
       "2            http://adec.epfl.ch\n",
       "3            http://aesv.epfl.ch\n",
       "4           http://aphys.epfl.ch"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_urls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def links_in_page(domain_url, page_url):\n",
    "    # valid url links in the page\n",
    "    links = []\n",
    "    # initial empty error message\n",
    "    error_message = None\n",
    "    \n",
    "    # We want the url to have the format http://www.example.com/\n",
    "    # and not http://www.example.com => to end with a forward slash\n",
    "    forward_slash = '/'\n",
    "    if(domain_url[-1:] != forward_slash):\n",
    "        domain_url += forward_slash\n",
    "    try:\n",
    "        # Http response for the requested url\n",
    "        resp = requests.get(page_url)\n",
    "        # Initialize parser for the requested url\n",
    "        soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "        \n",
    "        if(resp.ok):\n",
    "            # Pattern to match strings starting\n",
    "            # with a single forward slash '/...', a hastag '#...', 'http://' or 'https://'\n",
    "            pattern = '^(/[^/].|#|'+ domain_url + '.+)'\n",
    "            # pattern = '^(/[^/].|'+ domain_url+ '.+)'\n",
    "            # associated regex object\n",
    "            regObj = re.compile(pattern)\n",
    "            # Find valid url links in the page_url\n",
    "            for elem in soup.find_all(href=True):\n",
    "                tag = 'href'\n",
    "                link = elem[tag]\n",
    "                # matching string\n",
    "                result = regObj.match(link)\n",
    "                # If link matches '#' pattern\n",
    "                hashtag = '#'\n",
    "                # If link matches domain url pattern\n",
    "                prefix = domain_url\n",
    "                # If link matches '/' pattern\n",
    "                forward_slash = '/'\n",
    "                if (result is not None) and (result.group() == hashtag):\n",
    "                    # construct full url\n",
    "                    full_url = domain_url + result.string\n",
    "                    # add url to the list\n",
    "                    links.append(full_url)\n",
    "                elif (result is not None) and (prefix in result.group()):\n",
    "                    # add url to the list\n",
    "                    links.append(result.string)\n",
    "                elif (result is not None) and (result.group()[0] == forward_slash):\n",
    "                    # conctruct full url\n",
    "                    full_url = domain_url[:-1] + result.string\n",
    "                    # add url to the list\n",
    "                    links.append(full_url)\n",
    "            else:\n",
    "                error_message = resp.status_code\n",
    "    except requests.exceptions.InvalidURL as e:\n",
    "        error_message = 'The URL provided was somehow invalid.'\n",
    "    except requests.exceptions.URLRequired as e:\n",
    "        error_message = 'A valid URL is required to make a request.'\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        error_message = 'An HTTP error occurred.'\n",
    "    except requests.exceptions.Timeout as e:\n",
    "        error_message = 'The request timed out'\n",
    "    except requests.exceptions.ConnectionError as e:\n",
    "        error_message = 'A Connection error occurred.'\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        error_message = 'There was an ambiguous exception that occurred while handling your request.'\n",
    "    return set(links), error_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function that recursively explores a domain and find all pages in the domain\n",
    "def explore_domain(domain_url, page_url, visited, depth, max_depth=10):\n",
    "    # Construct list of full path urls within the page\n",
    "    links, error_message = links_in_page(domain_url, page_url)\n",
    "    \n",
    "    # update list of visited url\n",
    "    if(domain_url == page_url and depth == 0):\n",
    "        visited.append(domain_url)\n",
    "    for url in links:\n",
    "        # Check whether the url has never been visited\n",
    "        if not any(url == v for v in visited):\n",
    "            # If true add the new url reference to the list of visited pages\n",
    "            visited.append(url)\n",
    "    \n",
    "    index = visited.index(page_url)\n",
    "    # Stopping condition\n",
    "    if(depth == max_depth):\n",
    "        return visited\n",
    "    # Stopping condition (No more new url page to explore)\n",
    "    elif(index == len(visited)-1):\n",
    "        return visited\n",
    "    else:\n",
    "        return explore_domain(domain_url, visited[index + 1], visited, depth + 1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def myEpflGalleryBox_documents(domain_urls):\n",
    "    prefix_1 = 'https://documents.epfl.ch/'\n",
    "    prefix_2 = 'http://documents.epfl.ch/'\n",
    "    \n",
    "    # Assumption : Documents we are looking for are all children of the class 'myEpflGalleryBox'\n",
    "    class_ = 'myEpflGalleryBox'\n",
    "    # final dataframe to store the result\n",
    "    df_res = pd.DataFrame()\n",
    "    # Get all documents inside myEpflGalleyBox from the domain_urls\n",
    "    for domain_url in domain_urls:\n",
    "        print('domain url --> ', domain_url)\n",
    "        page_url = domain_url\n",
    "        visited = []\n",
    "        depth = 0\n",
    "        for page_url in explore_domain(domain_url, page_url, visited, depth):\n",
    "            error_message = None\n",
    "            try:\n",
    "                # Http response for the requested url\n",
    "                resp = requests.get(page_url)\n",
    "                # Initialize parser for the requested url\n",
    "                soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "\n",
    "                # Initialize empty list of all documents inside a myEpflGalleryBox\n",
    "                documents = []\n",
    "                # Initialize empty list of all document sizes inside a myEpflGalleryBox\n",
    "                sizes = []\n",
    "                if(resp.ok):\n",
    "                    # Get all galleries in the page\n",
    "                    galleries = soup.find_all(class_=class_)\n",
    "                    for box in galleries:\n",
    "                        # external links\n",
    "                        ext_links = box.find_all(href=True)\n",
    "                        for elem in ext_links:\n",
    "                            link = elem['href']\n",
    "                            if prefix_1 in link or prefix_2 in link:\n",
    "                                documents.append(link)\n",
    "                                try:\n",
    "                                    # Http response for the requested document\n",
    "                                    resp = requests.get(link, stream=True)\n",
    "                                    # helper\n",
    "                                    byte_to_kilobyte = 10**(-3)\n",
    "                                    # Get the size of the document\n",
    "                                    sizes.append(int(resp.headers['content-length']) * byte_to_kilobyte)\n",
    "                                except requests.exceptions.RequestException as e:\n",
    "                                    sizes.append(float('NaN'))\n",
    "                                except:\n",
    "                                    sizes.append(float('NaN'))\n",
    "                        \n",
    "\n",
    "                    # Update DataFrame of result\n",
    "                    d = {'domain': [domain_url]*len(documents), 'page': [page_url]*len(documents), 'document': documents, 'size [KB]': sizes}\n",
    "                    temp = pd.DataFrame(data=d)\n",
    "                    df_res = pd.concat([df_res, temp], axis=0)\n",
    "                else:\n",
    "                    error_message = resp.status_code\n",
    "            except requests.exceptions.InvalidURL as e:\n",
    "                error_message = 'The URL provided was somehow invalid.'\n",
    "            except requests.exceptions.URLRequired as e:\n",
    "                error_message = 'A valid URL is required to make a request.'\n",
    "            except requests.exceptions.HTTPError as e:\n",
    "                error_message = 'An HTTP error occurred.'\n",
    "            except requests.exceptions.Timeout as e:\n",
    "                error_message = 'The request timed out'\n",
    "            except requests.exceptions.ConnectionError as e:\n",
    "                error_message = 'A Connection error occurred.'\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                error_message = 'There was an ambiguous exception that occurred while handling your request.'\n",
    "            except exceptions.baseException as e:\n",
    "                error_message = 'An error occured.'\n",
    "    # Remove duplicates from dataframe of documents\n",
    "    return df_res.set_index('domain').drop_duplicates(subset=['page', 'document'])[['page', 'document', 'size [KB]']]         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "domain url -->  http://epsl.epfl.ch\n",
      "domain url -->  http://exploitation-energies.epfl.ch\n",
      "domain url -->  http://gel.epfl.ch\n",
      "domain url -->  http://gr-cel.epfl.ch\n",
      "domain url -->  http://hcf.epfl.ch\n",
      "domain url -->  http://ic.epfl.ch\n",
      "domain url -->  http://jahia.epfl.ch\n",
      "domain url -->  http://lacus.epfl.ch\n",
      "domain url -->  http://lamu.epfl.ch\n",
      "domain url -->  http://lashuel-lab.epfl.ch\n",
      "domain url -->  http://lasig.epfl.ch\n",
      "domain url -->  http://lbni.epfl.ch\n",
      "domain url -->  http://lbs.epfl.ch\n",
      "domain url -->  http://lch.epfl.ch\n",
      "domain url -->  http://lepa.epfl.ch\n",
      "domain url -->  http://lhtc.epfl.ch\n",
      "domain url -->  http://library.epfl.ch\n",
      "domain url -->  http://lifmet.epfl.ch\n",
      "domain url -->  http://limno.epfl.ch\n",
      "domain url -->  http://lingner-lab.epfl.ch\n",
      "domain url -->  http://linx.epfl.ch\n",
      "domain url -->  http://lipid.epfl.ch\n",
      "domain url -->  http://lmis4.epfl.ch\n",
      "domain url -->  http://lms.epfl.ch\n",
      "domain url -->  http://lnb.epfl.ch\n",
      "domain url -->  http://lnco.epfl.ch\n",
      "domain url -->  http://lne.epfl.ch\n",
      "domain url -->  http://lpdc.epfl.ch\n",
      "domain url -->  http://lqm.epfl.ch\n",
      "domain url -->  http://lsms.epfl.ch\n",
      "domain url -->  http://metamedia.epfl.ch\n",
      "domain url -->  http://mir.epfl.ch\n",
      "domain url -->  http://oche.epfl.ch\n",
      "domain url -->  http://phd.epfl.ch\n",
      "domain url -->  http://pixe.epfl.ch\n",
      "domain url -->  http://plte.epfl.ch\n",
      "domain url -->  http://polyjapan.epfl.ch\n",
      "domain url -->  http://quartier-nord.epfl.ch\n",
      "domain url -->  http://rolexlearningcenter.epfl.ch\n",
      "domain url -->  http://sber.epfl.ch\n",
      "domain url -->  http://scc.epfl.ch\n",
      "domain url -->  http://schoonjans-lab.epfl.ch\n",
      "domain url -->  http://sfi.epfl.ch\n",
      "domain url -->  http://smal.epfl.ch\n",
      "domain url -->  http://smlms.epfl.ch\n",
      "domain url -->  http://space.epfl.ch\n",
      "domain url -->  http://sti-ateliers.epfl.ch\n",
      "domain url -->  http://sv.epfl.ch\n",
      "domain url -->  http://sv-postdoc.epfl.ch\n",
      "domain url -->  http://sv-safety.epfl.ch\n",
      "domain url -->  http://tne.epfl.ch\n",
      "domain url -->  http://tronolab.epfl.ch\n",
      "domain url -->  http://unfold.epfl.ch\n",
      "domain url -->  http://valais.epfl.ch\n",
      "domain url -->  http://www.crem.ch\n",
      "domain url -->  http://yuva.epfl.ch\n"
     ]
    }
   ],
   "source": [
    "result = myEpflGalleryBox_documents(list(df_urls['URL']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "basename_index = 0\n",
    "extension_index = 1\n",
    "# input filename\n",
    "base = os.path.splitext(input_)[basename_index]\n",
    "# input file extension\n",
    "extension = os.path.splitext(input_)[extension_index]\n",
    "# Date the result as been produced (Specific format)\n",
    "date = datetime.datetime.now().strftime('%y%m%d.%H%M')\n",
    "# part of the output filename\n",
    "out = 'out'\n",
    "dot = '.'\n",
    "# output filename\n",
    "output_filename = base + dot + out + dot + date + extension\n",
    "# Write result to output file\n",
    "result.reset_index().to_csv(output_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(result.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#domain_url = 'http://lacus.epfl.ch'\n",
    "#page_url = domain_url\n",
    "#visited = []\n",
    "#depth = 0\n",
    "#tutu = explore_domain(domain_url, page_url, visited, depth)\n",
    "#toto = myEpflGalleryBox_documents([domain_url])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
